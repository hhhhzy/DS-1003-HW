{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "### Assignment Owner: Tian Wang, Marylou GabriÃ©\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Feature normalization\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size(num_instances, num_features)\n",
    "        test - test set, a 2D numpy array of size(num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "    \"\"\"\n",
    "    for i in range(train.shape[1]):\n",
    "        x_max = train[:,i].max()\n",
    "        x_min = train[:,i].min()        \n",
    "        # a is the rescale parameter, b is the shift parameter\n",
    "        # a, b are trained from the training set\n",
    "        # if the feature is constant, then do nothing to it\n",
    "        if x_max == x_min:\n",
    "            a = 1\n",
    "            b = 0\n",
    "        else:\n",
    "            a = 1/(x_max-x_min)\n",
    "            b = -x_min\n",
    "        \n",
    "        # apply the transformation on the train and test sets\n",
    "        train[:,i] = a*(train[:,i]+b)\n",
    "        test[:,i] = a*(test[:,i]+b)\n",
    "        train_normalized = train\n",
    "        test_normalized = test\n",
    "    \n",
    "    return train_normalized, test_normalized\n",
    "\n",
    "#######################################\n",
    "### The square loss function\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    norm = np.linalg.norm(X.dot(theta) - y)\n",
    "    loss = norm**2/(m)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "#######################################\n",
    "### The gradient of the square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss(as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    grad = 2*(X.T.dot(X).dot(theta) - X.T.dot(y))/m\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Gradient checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm. Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "(e_1 =(1,0,0,...,0), e_2 =(0,1,0,...,0), ..., e_d =(0,...,0,1))\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "(J(theta + epsilon * e_i) - J(theta - epsilon * e_i)) /(2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicating whether the gradient is correct or not\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    #TODO\n",
    "    for i in range(num_features):\n",
    "        e_i = np.zeros(num_features)\n",
    "        e_i[i] = 1\n",
    "        approx_grad[i] = (compute_square_loss(X, y, theta+epsilon*e_i) - \n",
    "                         compute_square_loss(X, y, theta-epsilon*e_i)) / (2*epsilon)   \n",
    "    # If the gradient is wrong, then return 0 \n",
    "    if np.linalg.norm(true_gradient - approx_grad) > tolerance:\n",
    "        indicator = 0\n",
    "    # if the gradient is corret, then return 1\n",
    "    else:\n",
    "        indicator = 1\n",
    "    \n",
    "    return indicator\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Generic gradient checker\n",
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, \n",
    "                             epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. \n",
    "    And check whether gradient_func(X, y, theta) returned the true \n",
    "    gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    true_gradient = gradient_func(X, y, theta)\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features)\n",
    "    #TODO\n",
    "    for i in range(num_features):\n",
    "        e_i = np.zeros(num_features)\n",
    "        e_i[i] = 1\n",
    "        approx_grad[i] = (objective_func(X, y, theta+epsilon*e_i) - \n",
    "                         objective_func(X, y, theta-epsilon*e_i)) / (2*epsilon)   \n",
    "    # If the gradient is wrong, then return 0 \n",
    "    if np.linalg.norm(true_gradient - approx_grad) > tolerance:\n",
    "        indicator = 0\n",
    "    # if the gradient is corret, then return 1\n",
    "    else:\n",
    "        indicator = 1\n",
    "    \n",
    "    return indicator\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Batch gradient descent\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array,(num_step+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    #TODO\n",
    "    loss_hist[0] = compute_square_loss(X, y, theta)\n",
    "    for i in range(num_step):\n",
    "        grad = compute_square_loss_gradient(X, y, theta)\n",
    "        if grad_check == True:\n",
    "            check = grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4)\n",
    "        theta = theta - alpha*grad       \n",
    "        theta_hist[i+1,:] = theta\n",
    "        loss = compute_square_loss(X, y, theta)\n",
    "        loss_hist[i+1] = loss\n",
    "    \n",
    "    return loss_hist, theta_hist\n",
    "\n",
    "\n",
    "#######################################\n",
    "### The gradient of regularized batch gradient descent\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized average square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Regularized batch gradient descent\n",
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_step - number of steps to run\n",
    "    \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step+1) is theta_hist[-1]\n",
    "        loss hist - the history of average square loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "    #TODO\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Stochastic gradient descent\n",
    "def stochastic_grad_descent(X, y, alpha=0.01, lambda_reg=10**-2, num_epoch=1000, eta0=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - string or float, step size in gradient descent\n",
    "                NOTE: In SGD, it's not a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every step is the float.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t).\n",
    "                if alpha == \"1/t\", alpha = 1/t.\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_epoch - number of epochs to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size(num_epoch, num_instances, num_features)\n",
    "                     for instance, theta in epoch 0 should be theta_hist[0], theta in epoch(num_epoch) is theta_hist[-1]\n",
    "        loss hist - the history of loss function vector, 2D numpy array of size(num_epoch, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) #Initialize theta\n",
    "\n",
    "    theta_hist = np.zeros((num_epoch, num_instances, num_features)) #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_epoch, num_instances)) #Initialize loss_hist\n",
    "    #TODO\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('ridge_regression_dataset.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n",
      "Split into Train and Test\n",
      "Scaling all to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.ones(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dot(theta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_func = compute_square_loss\n",
    "gradient_func = compute_square_loss_gradient\n",
    "\n",
    "generic_gradient_checker(X_train, y_train, theta, objective_func, gradient_func, \n",
    "                             epsilon=0.01, tolerance=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-245-f601215e3793>:82: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = 2*(X.T.dot(X).dot(theta) - X.T.dot(y))/m\n",
      "<ipython-input-251-789ba71ca009>:18: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta = theta - alpha*grad\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "alpha=0.1 \n",
    "num_step=1000\n",
    "grad_check=False\n",
    "\n",
    "\n",
    "num_instances, num_features = X.shape[0], X.shape[1]\n",
    "theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "theta = np.zeros(num_features)  #Initialize theta\n",
    "#TODO\n",
    "loss_hist[0] = compute_square_loss(X, y, theta)\n",
    "for i in range(num_step):\n",
    "    grad = compute_square_loss_gradient(X, y, theta)\n",
    "    if grad_check == True:\n",
    "        check = grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4)\n",
    "    theta = theta - alpha*grad       \n",
    "    theta_hist[i+1,:] = theta\n",
    "    loss = compute_square_loss(X, y, theta)\n",
    "    loss_hist[i+1] = loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "alpha=0.01 \n",
    "num_step=1000\n",
    "grad_check=False\n",
    "num_instances, num_features = X.shape[0], X.shape[1]\n",
    "theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "theta = np.zeros(num_features)  #Initialize theta\n",
    "\n",
    "\n",
    "grad = compute_square_loss(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = compute_square_loss_gradient(X, y, theta)\n",
    "theta = theta - alpha*grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01557088,  0.00047702,  0.01067574,  0.01394675, -0.00179555,\n",
       "        0.00028419,  0.00265257,  0.00265257,  0.0121825 ,  0.02092759,\n",
       "        0.02340332,  0.01091868, -0.00311404, -0.01082488,  0.02243974,\n",
       "        0.02663839,  0.02163764,  0.00445784,  0.00148609,  0.00148609,\n",
       "        0.00148609,  0.00476954,  0.00476954,  0.00476954,  0.00569285,\n",
       "        0.00569285,  0.00569285,  0.00611209,  0.00611209,  0.00611209,\n",
       "        0.00634201,  0.00634201,  0.00634201,  0.0087988 ,  0.0087988 ,\n",
       "        0.0087988 ,  0.00967221,  0.00967221,  0.00967221,  0.00850654,\n",
       "        0.00850654,  0.00850654,  0.00796286,  0.00796286,  0.00796286,\n",
       "        0.00765993,  0.00765993,  0.00765993, -0.02027466])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
