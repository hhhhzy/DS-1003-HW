{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Zheyuan Hu\n",
    "    \n",
    "NetId: zh2095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalence of ERM and probabilistic approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic loss function is \n",
    "\n",
    "$$\\ell_{\\text{logistic}}(y, w)=\\log\\left(1+\\exp(-yw^Tx)\\right)$$\n",
    "\n",
    "Then the objective funciton of $ERM$ for logistic loss is given by\n",
    "\n",
    "$$\\hat R_n(w) = \\frac 1 n \\sum_{i=1}^{n} \\log\\left(1+\\exp(-y_iw^Tx_i)\\right)$$\n",
    "\n",
    "Define the Bernoulli indicator as\n",
    "\n",
    "$$y_i' = \\begin{cases} 1 & y_i = 1 \\\\\n",
    "0 & y_i = -1 \\end{cases}$$\n",
    "\n",
    "Then we have the negative log-likelihood to be\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "NLL_{D}(w) &=& - \\sum_{i=1}^{n} y_i' \\log \\frac{1}{1+\\exp(-w^Tx_i)} + (1-y_i') \\log(1-\\frac{1}{1+\\exp(-w^Tx_i)} \\\\\n",
    "&=& \\sum_{i=1}^{n} - y_i' \\log\\frac{1}{1+\\exp(-w^Tx_i)} + (y_i'-1) \\log(1-\\frac{1}{1+\\exp(-w^Tx_i)})\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "When $y_i = 1$, that is, $y_i' = 1$,\n",
    "\n",
    "$$ \\log\\left(1+\\exp(-y_iw^Tx_i)\\right) = - y_i' \\log\\frac{1}{1+\\exp(-w^Tx_i)} = \\log\\left(1+\\exp(-w^Tx_i)\\right) $$\n",
    "\n",
    "When $y_i = -1$, that is, $y_i' = 0$,\n",
    "\n",
    "$$ \\log\\left(1+\\exp(-y_iw^Tx_i)\\right) = (y_i'-1) \\log(1-\\frac{1}{1+\\exp(-w^Tx_i)}) =  \\log\\left(1+\\exp(w^Tx_i)\\right) $$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$NLL_{D}(w) = n \\hat R_n(w)$$\n",
    "\n",
    "Thus, the two approaches are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Separable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict $y$ by \n",
    "\n",
    "$$\\hat{y} = \\text{sign}(x^Tw)$$\n",
    "\n",
    "that is, for all $i = 1,2,...,n$, we predict $y_i$ by \n",
    "\n",
    "$$\\hat y_i {x_i}^Tw > 0$$\n",
    "\n",
    "Therefore, the decision boundary of logistic regression is given by $\\left\\{x\\colon x^Tw=0\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $y = 1$, the likelihood function with respect to $c\\hat w$ is given by\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "L(c\\hat w) &=& p(y=1 \\mid x; c\\hat w) \\\\\n",
    "&=& 1 / (1 + e^{-x^T c\\hat w)} \\\\\n",
    "&=& (1 + e^{-x^T\\hat w\\cdot c})^{-1}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Take derivative of $L(c\\hat w)$ with respect to $c$, we get\n",
    "\n",
    "$$\\begin{eqnarray*} \n",
    "\\frac {dL}{dc} &=& -(1 + e^{-x^T\\hat w c})^{-2} \\cdot (-x^Tw \\cdot e^{-x^Tw c}) \\\\\n",
    "&=& x^Tw \\cdot e^{-x^Tw c} \\cdot (1 + e^{-x^T\\hat w c})^{-2} \n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Since all examples are classified correctly,\n",
    "\n",
    "$$x^Tw = y = 1 > 0$$\n",
    "\n",
    "And we also have \n",
    "\n",
    "$$e^{-x^Tw c} > 0,  (1 + e^{-x^T\\hat w c})^{-2} >0$$\n",
    "\n",
    "That is, \n",
    "\n",
    "$$\\frac {dL}{dc} > 0$$\n",
    "\n",
    "Similarly, $\\frac {dL}{dc} > 0$ also holds for $ y = 0$\n",
    "\n",
    "Therefore, as $c$ increases, the likelihood of the data would always increase, which means that MLE is not well-defined in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray*}\n",
    "J_{\\text{logistic}}(w) & = & \\hat{R}_{n}(w)+\\lambda\\|w\\|^{2}\\\\\n",
    " & = & \\frac{1}{n}\\sum_{i=1}^{n}\\log\\left(1+\\exp\\left(-y^{(i)}w^{T}x^{(i)}\\right)\\right)+\\lambda\\|w\\|^{2}.\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "From notes 3.1.3, we know that $\\exp\\left(-y^{(i)}w^{T}x^{(i)}\\right)$  is convex ,\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\exp\\left(-y^{(i)}w^{T}x^{(i)}\\right) \\text{ is convex } &\\Longrightarrow& 1+\\exp\\left(-y^{(i)}w^{T}x^{(i)}\\right) \\text{ is convex } \\\\\n",
    "&\\Longrightarrow& \\log\\left(1+\\exp\\left(-y^{(i)}w^{T}x^{(i)}\\right)\\right) \\text{ is convex } \\\\\n",
    "&\\Longrightarrow&  \\hat R_n(w) \\text{ is convex } \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Also From notes 3.1.3, we know every norm is convex, so\n",
    "$$\\lambda\\|w\\|^{2} \\text{ is convex }$$\n",
    "\n",
    "Thus, the objective function $J_{\\text{logistic}}(w)$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_objective(theta, X, y, l2_param=1):\n",
    "    '''\n",
    "    Args:\n",
    "        theta: 1D numpy array of size num_features\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        l2_param: regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        objective: scalar value of objective function\n",
    "    '''\n",
    "    num_instances = X.shape[0]\n",
    "    num_features = X.shape[1]   \n",
    "    sum_risk = 0   \n",
    "    for i in range(num_instances):\n",
    "        sum_risk += np.logaddexp(0, -y[i]*np.dot(theta,X[i]))\n",
    "    \n",
    "    risk = sum_risk/num_instances\n",
    "    reg = l2_param * np.dot(theta,theta)\n",
    "    objective = risk + reg\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "X_train = np.genfromtxt('./logistic-code/X_train.txt', delimiter = ',')\n",
    "X_val = np.genfromtxt('./logistic-code/X_val.txt', delimiter = ',')\n",
    "y_train = np.genfromtxt('./logistic-code/y_train.txt', delimiter = ',')\n",
    "y_val = np.genfromtxt('./logistic-code/y_val.txt', delimiter = ',')\n",
    "\n",
    "# change label space from {0,1} to {-1,1}\n",
    "y_train[y_train==0] = -1\n",
    "y_val[y_val==0] = -1\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "# add bias term\n",
    "bias_train = np.ones([X_train.shape[0], 1])\n",
    "bias_val = np.ones([X_val.shape[0], 1])\n",
    "X_train = np.hstack((X_train, bias_train))\n",
    "X_val = np.hstack((X_val, bias_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_reg(X, y, objective_function, l2_param=1):\n",
    "    '''\n",
    "    Args:\n",
    "        X: 2D numpy array of size (num_instances, num_features)\n",
    "        y: 1D numpy array of size num_instances\n",
    "        objective_function: function returning the value of the objective\n",
    "        l2_param: regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        optimal_theta: 1D numpy array of size num_features\n",
    "    '''\n",
    "    num_features = X.shape[1]\n",
    "    theta = np.ones(num_features)\n",
    "    optimal_theta = minimize(objective_function, theta).x  \n",
    "    \n",
    "    return optimal_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "X = X_train\n",
    "y = y_train\n",
    "objective_function = partial(f_objective, X=X, y=y, l2_param=1)\n",
    "optimal_theta = fit_logistic_reg(X, y, objective_function, l2_param=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00095626, -0.00029962,  0.0030268 ,  0.10532762, -0.00358736,\n",
       "       -0.0013585 , -0.00385288, -0.00079013, -0.00114407, -0.07178432,\n",
       "        0.00654892, -0.00451097,  0.01124928, -0.00386437, -0.00271262,\n",
       "        0.00150363, -0.00278399, -0.0091906 , -0.00682276, -0.01027486,\n",
       "        0.00281868])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_param_set = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "log_L_set = []\n",
    "\n",
    "# train the model for each l2_param\n",
    "for l2_param in l2_param_set:\n",
    "    objective_function = partial(f_objective, X=X_train, y=y_train, l2_param=l2_param)\n",
    "    optimal_theta = fit_logistic_reg(X_train, y_train, objective_function, l2_param)\n",
    "    \n",
    "    # calculate the log loss on the validation set with the optimal theta\n",
    "    log_L = 0    \n",
    "    for i in range(X_val.shape[0]):\n",
    "        log_L += np.logaddexp(0, -y_val[i]*np.dot(optimal_theta,X_val[i]))    \n",
    "    log_L_set.append(log_L)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the log-likelihood for different values of the regularization parameter\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('l2_parameter')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.plot(l2_param_set, log_L_set, 'x-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularization parameter that minimizes the log-likelihood on the validation data is: \n",
      "l2_param =  0.01\n"
     ]
    }
   ],
   "source": [
    "best_param = l2_param_set[np.argmin(log_L_set)]\n",
    "print('The regularization parameter that minimizes the log-likelihood on the validation data is: ')\n",
    "print('l2_param = ', best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Logistic Regression with Gaussian Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Bayes rule, we can write the posterior distribution as\n",
    "\n",
    "$$ p(w|D) = \\frac{p(D|w) \\cdot p(w)}{p(D)}$$\n",
    "\n",
    "Consider both sides as function of $w$, for fixed D, then\n",
    "\n",
    "$$ p(w|D) = c \\cdot p(D|w) \\cdot p(w), \\space \\text{ for some constant } c = \\frac {1}{p(D)}$$\n",
    "\n",
    "Also,\n",
    "\n",
    "$$ p(D|w) = L_D(w) = \\exp(-NLL_D(w))$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ p(w|D) = c \\cdot \\exp(-NLL_D(w)) \\cdot p(w), \\space \\text{ where constant } c = \\frac {1}{p(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the Gaussian $p(w)$ is not a conjugate prior. \n",
    "\n",
    "From Q9, we know that $p(w|D) = c \\cdot \\exp(-NLL_D(w)) \\cdot p(w)$, where the likelihood function $\\exp(-NLL_D(w))$ is logistic and $p(w)$ is Gaussian.\n",
    "\n",
    "Therefore, $\\exp(-NLL_D(w)) \\cdot p(w)$ will never be Gaussian, that is, the posterior $p(w|D)$ is not in the same family(Gaussian) as $p(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $MAP$ for $w$ minimizes the negative log posterior of $w$ and the regularized logistic function:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\hat w_{MAP} &=& argmin(-\\log p(w|D)) \\\\\n",
    "&=& argmin \\frac 1 n \\sum_{i=1}^{n} \\log (1+\\exp (y_iw^Tx_i)) + \\lambda\\|w\\|^{2} &&&&& (1)\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "From Q10, we have that\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "-\\log p(w|D)) &=& -\\log( c \\cdot \\exp(-NLL_D(w)) \\cdot p(w))\\\\\n",
    "&=& -\\log c + NLL_D(w) - \\log (p(w)) \\\\ \n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "where $c$ is a constant $NLL_D(w) = n \\cdot \\hat R_n(w)$\n",
    "\n",
    "Given the piror $p(w)$ is Gaussian,\n",
    "\n",
    "$$\\log (p(w)) = -\\frac 1 2 \\log(|2 \\pi \\Sigma|) + \\frac 1 2 w^T \\Sigma^{-1} w$$\n",
    "\n",
    "Want to find $\\Sigma$ such that equation (1) holds.\n",
    "\n",
    "Notice that the only term that match with $\\frac 1 2 w^T \\Sigma^{-1} w$ is $\\lambda\\|w\\|^{2}$, and since $NLL_D(w) = n \\cdot \\hat R_n(w)$,  we need to have\n",
    "\n",
    "$$\\frac 1 2 w^T \\Sigma^{-1} w = n \\cdot \\lambda\\|w\\|^{2} $$\n",
    "\n",
    "Solve for $\\Sigma$, we get\n",
    "\n",
    "$$\\Sigma = \\frac {1} {2 n \\lambda} I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have the $ERM$ equal to the $MAP$ estimator, we need\n",
    "\n",
    "$$\\Sigma = \\frac {1} {2 n \\lambda} I$$\n",
    "\n",
    "Here we choose $\\Sigma  = I$, then \n",
    "\n",
    "$$\\lambda = \\frac {1} {2n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray*}\n",
    "p(x=H|\\theta_1, \\theta_2) &=& p(x=H,z=H|\\theta_1, \\theta_2) + p(x=H,z=T|\\theta_1, \\theta_2) \\\\\n",
    "&=& p(x=H|z=H, \\theta_2) \\cdot p(z=H|\\theta_1) +  p(x=H|z=T, \\theta_2) \\cdot p(z=T|\\theta_1) \\\\\n",
    "&=& \\theta_2 \\cdot \\theta_1 + 0 \\cdot (1-\\theta_1) \\\\\n",
    "&=& \\theta_1 \\theta_2\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of $D_r$ as a function of $\\theta_1$ and $\\theta_2$:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "p(D_r|\\theta_1, \\theta_2) &=& {N_r \\choose n_h} \\cdot (\\theta_1 \\theta_2)^{n_h} \\cdot (1-\\theta_1 \\theta_2)^{n_t}\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, we only have a dataset with reported results where head is reported with probability $\\theta_1 \\theta_2$ and tail is reported with $1 - \\theta_1 \\theta_2$. This means that we can only use $MLE$ to estimate the parameter $\\theta_1 \\theta_2$, but not $\\theta_1$ and $\\theta_2$ seperately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood function on the two datasets is:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "L(\\theta_1, \\theta_2) &=& p(D_r,D_c|\\theta_1, \\theta_2) \\\\\n",
    "&=& p(D_r|\\theta_1, \\theta_2) \\cdot p(D_c|\\theta_1) &(D_c \\text{ only depends on } \\theta_1)\\\\\n",
    "&=& {N_r \\choose n_h} \\cdot (\\theta_1 \\theta_2)^{n_h} \\cdot (1-\\theta_1 \\theta_2)^{n_t} \\cdot {N_c \\choose c_h} \\cdot \\theta_{1} ^{c_h} \\cdot (1-\\theta_1 )^{c_t}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Then the log likelihood function is:\n",
    "\n",
    "$$ \\log(L(\\theta_1, \\theta_2)) = \\log{N_r \\choose n_h} + \\log{N_c \\choose c_h} + n_h \\log(\\theta_1 \\theta_2) + n_t \\log(1-\\theta_1 \\theta_2) + c_h \\log\\theta_{1} + c_t \\log(1-\\theta_1)$$\n",
    "\n",
    "We want to find $\\theta_1$ and $\\theta_2$ that maximize the log likelihood, so we take the partial derivatives for $\\theta_1$ and $\\theta_2$:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\frac{\\partial \\log(L(\\theta_1, \\theta_2))} {\\partial \\theta_1} &=& \\frac{n_h}{\\theta_1} - \\frac{n_t \\theta_2}{1-\\theta_1 \\theta_2} + \\frac{c_h}{\\theta_1} - \\frac {c_t}{1-\\theta_1} \\\\\n",
    "\\frac{\\partial \\log(L(\\theta_1, \\theta_2))} {\\partial \\theta_2} &=& \\frac{n_h}{\\theta_2} - \\frac{n_t \\theta_1}{1-\\theta_1 \\theta_2}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "To have both the partial derivatives equal to 0, we get\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\theta_1 \\theta_2 &=& \\frac {n_h}{n_h+n_t} \\\\\n",
    "\\theta_1 &=&  \\frac {c_h}{c_h+c_t}\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Further solve for $\\theta_2$, the $MLE$ estimate for $\\theta_1$ and $\\theta_2$ is:\n",
    "\n",
    "$$\\begin{eqnarray*}\n",
    "\\theta_1 &=& \\frac {c_h}{c_h+c_t} \\\\\n",
    "\\theta_2 &=& \\frac{n_h(c_h+c_t)}{c_h(n_h+n_t)}\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior distribution for $\\theta_1$ is:\n",
    "\n",
    "$$\\theta_1 \\sim Beta(h,t)$$\n",
    "\n",
    "After obtaining the dataset $D_c$, the posterior distribution becomes:\n",
    "\n",
    "$$\\theta_1 \\sim Beta(h+c_h,t+c_t)$$\n",
    "\n",
    "Then the $MAP$ estimate for $\\theta_1$ is the mode of the Beta distribution, which is\n",
    "\n",
    "$$\\theta_1 = \\frac {h+c_h-1}{h+c_h+t+c_t-2}$$\n",
    "\n",
    "From the previous question, we obtain that the $MLE$ for $\\theta_1 \\theta_2$ is\n",
    "\n",
    "$$\\theta_1 \\theta_2 = \\frac {n_h}{n_h+n_t}$$\n",
    "\n",
    "Solve for $\\theta_2$, the $MAP$ estimate for $\\theta_2$ is:\n",
    "\n",
    "$$\\theta_2 = \\frac{n_h(h+c_h+t+c_t-2)}{(n_h+n_t)(h+c_h-1)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
